{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a2e773f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "PyTorch version: 2.7.1+cu126\n",
      "CUDA device name: NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA device name:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40aade1",
   "metadata": {},
   "source": [
    "# Self Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "cc298ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input tensor: torch.Size([4, 64, 128])\n",
      "Shape of output tensor: torch.Size([4, 64, 128])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "sequence_length = 64\n",
    "embed_dim = 128\n",
    "x = torch.randn(batch_size, sequence_length, embed_dim)\n",
    "print(\"Shape of input tensor:\", x.shape)\n",
    "\n",
    "similarity = (x @ x.transpose(1, 2)) / (embed_dim ** 0.5)\n",
    "\n",
    "attention_matrix = similarity.softmax(dim=-1)\n",
    "\n",
    "output = attention_matrix @ x\n",
    "\n",
    "print(\"Shape of output tensor:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fceb90a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 64, 128])\n"
     ]
    }
   ],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, embedding_dimension):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embedding_dimension\n",
    "\n",
    "        self.query = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.key = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.value = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "\n",
    "        similarity = (q @ k.transpose(1,2)) / self.embed_dim**0.5\n",
    "        attention = similarity.softmax(axis=-1)\n",
    "        out = attention @ v\n",
    "\n",
    "        return out\n",
    "    \n",
    "rand = torch.rand(4, 64, 128)\n",
    "attn = Attention(embedding_dimension=128)\n",
    "output = attn(rand)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb2a6d6",
   "metadata": {},
   "source": [
    "## Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ff07215f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 64, 128])\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embedding_dimension, num_heads):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed_dim = embedding_dimension\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = self.embed_dim //self.num_heads\n",
    "        self.multihead_qkv = nn.ModuleList()\n",
    "\n",
    "        for head in range(self.num_heads):\n",
    "            qkv_proj = nn.ModuleDict([\n",
    "                [\"Q\", nn.Linear(self.embed_dim, self.head_dim)],\n",
    "                [\"K\", nn.Linear(self.embed_dim, self.head_dim)],\n",
    "                [\"V\", nn.Linear(self.embed_dim, self.head_dim)]\n",
    "            ])\n",
    "\n",
    "            self.multihead_qkv.append(qkv_proj)\n",
    "\n",
    "        self.proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        heads_out = []\n",
    "        for head in self.multihead_qkv:\n",
    "            q = head[\"Q\"](x)\n",
    "            k = head[\"K\"](x)\n",
    "            v = head[\"V\"](x)\n",
    "            \n",
    "            similarity = (q @ k.transpose(1,2)) / self.head_dim**0.5\n",
    "            attention = similarity.softmax(axis=-1)\n",
    "            output = attention @ v\n",
    "\n",
    "            heads_out.append(output)\n",
    "\n",
    "        heads_out = torch.cat(heads_out, dim=-1)\n",
    "\n",
    "        out = self.proj(heads_out)\n",
    "        return out\n",
    "    \n",
    "\n",
    "rand = torch.randn(4,64,128)\n",
    "attn = MultiHeadAttention(128, 2)\n",
    "out = attn(rand)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "046123bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 16, 128])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SelfAttentionEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, attn_p=0.0, proj_p=0.0, bias=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        if self.embed_dim % self.num_heads != 0:\n",
    "            raise ValueError(f\"Embedding dimension {self.embed_dim} must be divisible by number of heads {self.num_heads}\")\n",
    "        self.head_dim = self.embed_dim // self.num_heads\n",
    "\n",
    "        self.query = nn.Linear(self.embed_dim, self.embed_dim, bias = bias)\n",
    "        self.key = nn.Linear(self.embed_dim, self.embed_dim, bias = bias)\n",
    "        self.value = nn.Linear(self.embed_dim, self.embed_dim, bias = bias)\n",
    "        self.attn_drop = nn.Dropout(attn_p)\n",
    "\n",
    "        self.proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.proj_drop = nn.Dropout(proj_p)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, embed_dim = x.shape\n",
    "        if embed_dim != self.embed_dim:\n",
    "            raise ValueError(f\"Input embedding dimension {embed_dim} does not match model's expected dimension {self.embed_dim}\")\n",
    "        \n",
    "        # (batch_size, seq_len, embed_dim) reshaped to (batch_size, seq_len, num_heads, head_dim) transposed to (batch_size, num_heads, seq_len, head_dim)\n",
    "        q = self.query(x).reshape(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        k = self.key(x).reshape(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        v = self.value(x).reshape(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1,2)\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "        out = attn @ v\n",
    "\n",
    "        # (batch_size, num_heads, seq_len, head_dim) transposed to (batch_size, seq_len, num_heads, head_dim) reshaped to (batch_size, seq_len, embed_dim)    reverse the process\n",
    "        out = out.transpose(1, 2).reshape(batch_size, seq_len, self.embed_dim)\n",
    "        out = self.proj(out)\n",
    "        out = self.proj_drop(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "rand = torch.randn(4, 16, 128)\n",
    "attn = SelfAttentionEncoder(128, 2)\n",
    "output = attn(rand)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad35adf",
   "metadata": {},
   "source": [
    "## Padding Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5edcea9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method 1:\n",
      "--------\n",
      "tensor([[[[ True,  True,  True,  True, False, False]]]])\n",
      "tensor([[[[0.9137, 0.6858, 0.8030, 0.1105,   -inf,   -inf],\n",
      "          [0.5115, 0.3593, 0.1674, 0.7502,   -inf,   -inf],\n",
      "          [0.6505, 0.0342, 0.9939, 0.2707,   -inf,   -inf],\n",
      "          [0.1533, 0.9564, 0.7679, 0.0235,   -inf,   -inf],\n",
      "          [0.5737, 0.1570, 0.0986, 0.3515,   -inf,   -inf],\n",
      "          [0.7620, 0.3097, 0.0724, 0.0090,   -inf,   -inf]],\n",
      "\n",
      "         [[0.9492, 0.2097, 0.2889, 0.2773,   -inf,   -inf],\n",
      "          [0.7308, 0.7691, 0.6744, 0.4852,   -inf,   -inf],\n",
      "          [0.3991, 0.8991, 0.6009, 0.3711,   -inf,   -inf],\n",
      "          [0.8267, 0.2496, 0.6943, 0.1098,   -inf,   -inf],\n",
      "          [0.1869, 0.0369, 0.0927, 0.2785,   -inf,   -inf],\n",
      "          [0.6996, 0.4437, 0.6242, 0.7508,   -inf,   -inf]]]])\n",
      "Method 2:\n",
      "--------\n",
      "tensor([[[[ True,  True,  True,  True, False, False],\n",
      "          [ True,  True,  True,  True, False, False],\n",
      "          [ True,  True,  True,  True, False, False],\n",
      "          [ True,  True,  True,  True, False, False],\n",
      "          [ True,  True,  True,  True, False, False],\n",
      "          [ True,  True,  True,  True, False, False]]]])\n",
      "tensor([[[[0.9137, 0.6858, 0.8030, 0.1105,   -inf,   -inf],\n",
      "          [0.5115, 0.3593, 0.1674, 0.7502,   -inf,   -inf],\n",
      "          [0.6505, 0.0342, 0.9939, 0.2707,   -inf,   -inf],\n",
      "          [0.1533, 0.9564, 0.7679, 0.0235,   -inf,   -inf],\n",
      "          [0.5737, 0.1570, 0.0986, 0.3515,   -inf,   -inf],\n",
      "          [0.7620, 0.3097, 0.0724, 0.0090,   -inf,   -inf]],\n",
      "\n",
      "         [[0.9492, 0.2097, 0.2889, 0.2773,   -inf,   -inf],\n",
      "          [0.7308, 0.7691, 0.6744, 0.4852,   -inf,   -inf],\n",
      "          [0.3991, 0.8991, 0.6009, 0.3711,   -inf,   -inf],\n",
      "          [0.8267, 0.2496, 0.6943, 0.1098,   -inf,   -inf],\n",
      "          [0.1869, 0.0369, 0.0927, 0.2785,   -inf,   -inf],\n",
      "          [0.6996, 0.4437, 0.6242, 0.7508,   -inf,   -inf]]]])\n"
     ]
    }
   ],
   "source": [
    "### Create an example attention matrix (b x h x n x n) ###\n",
    "rand_attn = torch.rand(1,2,6,6) # I have 2 heads here!\n",
    "\n",
    "### Create Attention Mask in the shape (b x n) ###\n",
    "attention_mask = torch.tensor([1,1,1,1,0,0]).unsqueeze(0).bool()\n",
    "\n",
    "print(\"Method 1:\")\n",
    "print(\"--------\")\n",
    "### Add Two Extra Dimension for the (b x h x n x n) ###\n",
    "### So unsqueeze mask to be (b x 1 x 1 x n) ###\n",
    "attention_mask = attention_mask.unsqueeze(1).unsqueeze(1)\n",
    "\n",
    "### Unsqueezed with dummy broadcast dimension ###\n",
    "print(attention_mask)\n",
    "print(rand_attn.masked_fill_(~attention_mask, float(\"-inf\")))\n",
    "\n",
    "print(\"Method 2:\")\n",
    "print(\"--------\")\n",
    "### Repeat the Dummy Dimension for seq_len so attention mask is (b 1 x n x n) ###\n",
    "attention_mask = attention_mask.repeat(1,1,6,1) # repeat dummy middle dim 6 times (for the seq_len) \n",
    "print(attention_mask)\n",
    "print(rand_attn.masked_fill_(~attention_mask, float(\"-inf\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3667a1c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Mask:\n",
      "tensor([[ True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False]])\n",
      "Final Output: torch.Size([3, 5, 9])\n"
     ]
    }
   ],
   "source": [
    "class SelfAttentionPadding(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, attn_p=0.0, proj_p=0.0, bias=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        if self.embed_dim % self.num_heads != 0:\n",
    "            raise ValueError(f\"Embedding dimension {self.embed_dim} must be divisible by number of heads {self.num_heads}\")\n",
    "        self.head_dim = self.embed_dim // self.num_heads\n",
    "\n",
    "        self.query = nn.Linear(self.embed_dim, self.embed_dim, bias = bias)\n",
    "        self.key = nn.Linear(self.embed_dim, self.embed_dim, bias = bias)\n",
    "        self.value = nn.Linear(self.embed_dim, self.embed_dim, bias = bias)\n",
    "        self.attn_drop = nn.Dropout(attn_p)\n",
    "\n",
    "        self.proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.proj_drop = nn.Dropout(proj_p)\n",
    "\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        \n",
    "        batch_size, seq_len, embed_dim = x.shape\n",
    "        if embed_dim != self.embed_dim:\n",
    "            raise ValueError(f\"Input embedding dimension {embed_dim} does not match model's expected dimension {self.embed_dim}\")\n",
    "        \n",
    "        # (batch_size, seq_len, embed_dim) reshaped to (batch_size, seq_len, num_heads, head_dim) transposed to (batch_size, num_heads, seq_len, head_dim)\n",
    "        q = self.query(x).reshape(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        k = self.key(x).reshape(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        v = self.value(x).reshape(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1,2)\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        \n",
    "\n",
    "        #################################################################################\n",
    "        if attention_mask is not None:\n",
    "            # attention_mask = attention_mask.unsqueeze(1).unsqueeze(1)   # (batch_size, 1, 1, seq_len)\n",
    "            attention_mask = attention_mask.unsqueeze(1).unsqueeze(1).repeat(1, 1, seq_len, 1)  # repeat dummy middle dim for seq_len (above line also works, but this is better for flash attention)\n",
    "            attn = attn.masked_fill(~attention_mask, float(\"-inf\"))  # Apply the attention mask\n",
    "        #################################################################################\n",
    "\n",
    "        \n",
    "        attn = attn.softmax(dim=-1)\n",
    "        # print(\"Attention Matrix after softmax:\\n\", attn)\n",
    "        attn = self.attn_drop(attn)\n",
    "        out = attn @ v\n",
    "\n",
    "        # (batch_size, num_heads, seq_len, head_dim) transposed to (batch_size, seq_len, num_heads, head_dim) reshaped to (batch_size, seq_len, embed_dim)    reverse the process\n",
    "        out = out.transpose(1, 2).reshape(batch_size, seq_len, self.embed_dim)\n",
    "        out = self.proj(out)\n",
    "        out = self.proj_drop(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "### We will now have sequences of different lengths, identify the number of tokens in each sequence ###\n",
    "seq_lens = [3,5,4]\n",
    "embed_dim = 9\n",
    "num_heads = 3\n",
    "a = SelfAttentionPadding(embed_dim, num_heads)\n",
    "\n",
    "### Create a random tensor in the shape (Batch x Seq Len x Embed Dim) ###\n",
    "### This will be a tensor upto the max(seq_lens) ###\n",
    "rand = torch.randn(len(seq_lens),max(seq_lens),embed_dim)\n",
    "\n",
    "### Create Attention Mask from the seq_lens (shortest sequences padded to the longest ###\n",
    "masks = torch.nn.utils.rnn.pad_sequence([torch.ones(l) for l in seq_lens], batch_first=True, padding_value=0).bool()\n",
    "print(\"Attention Mask:\")\n",
    "print(masks)\n",
    "\n",
    "### Pass through MHA ###\n",
    "output = a(rand, attention_mask=masks)\n",
    "print(\"Final Output:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf695403",
   "metadata": {},
   "source": [
    "## Causal Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "298d7259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Causal Mask:\n",
      " tensor([[ True, False, False, False, False, False, False, False],\n",
      "        [ True,  True, False, False, False, False, False, False],\n",
      "        [ True,  True,  True, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True]])\n",
      "Padding Mask:\n",
      " tensor([[ True,  True,  True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False, False, False]])\n",
      "Combined Mask:\n",
      " tensor([[ True, False, False, False, False, False, False, False],\n",
      "        [ True,  True, False, False, False, False, False, False],\n",
      "        [ True,  True,  True, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False, False, False]])\n"
     ]
    }
   ],
   "source": [
    "seq_len = 8\n",
    "ones = torch.ones(seq_len, seq_len)\n",
    "causal_mask = torch.tril(ones).bool()\n",
    "print(\"Causal Mask:\\n\", causal_mask)\n",
    "\n",
    "padding_mask = torch.tensor([1, 1, 1, 1, 0, 0, 0, 0]).bool()\n",
    "padding_mask = padding_mask.unsqueeze(0).repeat(seq_len, 1)\n",
    "print(\"Padding Mask:\\n\", padding_mask)\n",
    "\n",
    "combined_mask = causal_mask.masked_fill(~padding_mask, False)\n",
    "print(\"Combined Mask:\\n\", combined_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6b6c0f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Mask:\n",
      "tensor([[ True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False]])\n",
      "Final Output: torch.Size([3, 5, 9])\n"
     ]
    }
   ],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, causal=True, attn_p=0.0, proj_p=0.0, bias=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        if self.embed_dim % self.num_heads != 0:\n",
    "            raise ValueError(f\"Embedding dimension {self.embed_dim} must be divisible by number of heads {self.num_heads}\")\n",
    "        self.head_dim = self.embed_dim // self.num_heads\n",
    "        self.causal = causal\n",
    "\n",
    "        self.query = nn.Linear(self.embed_dim, self.embed_dim, bias = bias)\n",
    "        self.key = nn.Linear(self.embed_dim, self.embed_dim, bias = bias)\n",
    "        self.value = nn.Linear(self.embed_dim, self.embed_dim, bias = bias)\n",
    "        self.attn_drop = nn.Dropout(attn_p)\n",
    "\n",
    "        self.proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.proj_drop = nn.Dropout(proj_p)\n",
    "\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        \n",
    "        batch_size, seq_len, embed_dim = x.shape\n",
    "        if embed_dim != self.embed_dim:\n",
    "            raise ValueError(f\"Input embedding dimension {embed_dim} does not match model's expected dimension {self.embed_dim}\")\n",
    "        \n",
    "        # (batch_size, seq_len, embed_dim) reshaped to (batch_size, seq_len, num_heads, head_dim) transposed to (batch_size, num_heads, seq_len, head_dim)\n",
    "        q = self.query(x).reshape(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        k = self.key(x).reshape(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        v = self.value(x).reshape(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        attn = (q @ k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "\n",
    "\n",
    "        #################################################################################\n",
    "        if self.causal:\n",
    "            causal_mask = torch.tril(torch.ones(seq_len, seq_len)).bool().to(x.device)\n",
    "            causal_mask = causal_mask.reshape(1, 1, seq_len, seq_len)\n",
    "            if attention_mask is not None:\n",
    "                attention_mask = attention_mask.unsqueeze(1).unsqueeze(1).repeat(1, 1, seq_len, 1)\n",
    "                causal_mask = causal_mask.repeat(batch_size, 1, 1, 1)  # repeat for batch size\n",
    "                causal_mask = causal_mask.masked_fill(~attention_mask, False)  # Apply padding mask to causal mask\n",
    "            \n",
    "            attn = attn.masked_fill(~causal_mask, float(\"-inf\"))  # Apply the causal masks\n",
    "        else:\n",
    "            if attention_mask is not None:\n",
    "                attention_mask = attention_mask.unsqueeze(1).unsqueeze(1).repeat(1, 1, seq_len, 1)\n",
    "                attn = attn.masked_fill(~attention_mask, float(\"-inf\"))  #\n",
    "        #################################################################################\n",
    "\n",
    "\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        # print(\"Attention Matrix after softmax:\\n\", attn)\n",
    "        attn = self.attn_drop(attn)\n",
    "        out = attn @ v\n",
    "\n",
    "        # (batch_size, num_heads, seq_len, head_dim) transposed to (batch_size, seq_len, num_heads, head_dim) reshaped to (batch_size, seq_len, embed_dim)    reverse the process\n",
    "        out = out.transpose(1, 2).reshape(batch_size, seq_len, self.embed_dim)\n",
    "        out = self.proj(out)\n",
    "        out = self.proj_drop(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "### We will now have sequences of different lengths, identify the number of tokens in each sequence ###\n",
    "seq_lens = [3,5,4]\n",
    "embed_dim = 9\n",
    "num_heads = 3\n",
    "a = SelfAttention(embed_dim, num_heads)\n",
    "\n",
    "### Create a random tensor in the shape (Batch x Seq Len x Embed Dim) ###\n",
    "### This will be a tensor upto the max(seq_lens) ###\n",
    "rand = torch.randn(len(seq_lens),max(seq_lens),embed_dim)\n",
    "\n",
    "### Create Attention Mask from the seq_lens (shortest sequences padded to the longest ###\n",
    "masks = torch.nn.utils.rnn.pad_sequence([torch.ones(l) for l in seq_lens], batch_first=True, padding_value=0).bool()\n",
    "print(\"Attention Mask:\")\n",
    "print(masks)\n",
    "\n",
    "### Pass through MHA ###\n",
    "output = a(rand, attention_mask=masks)\n",
    "print(\"Final Output:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894633f7",
   "metadata": {},
   "source": [
    "# Cross Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b127996f",
   "metadata": {},
   "source": [
    "In cross attention, the **query** comes from the decoder, while the **key** and **value** come from the encoder. This mechanism is essential in models like the Transformer, where the decoder needs to attend to the encoder's output to generate each token in the target sequence.\n",
    "\n",
    "**Example:**  \n",
    "When translating a sentence from English to French, the query is the French sentence being generated (decoder), and the key and value are the encoded representations of the English sentence (encoder).\n",
    "\n",
    "Unlike self-attention in the decoder, **causal masking is not used in cross attention**. This is because, during translation, the decoder should be able to attend to all tokens in the encoder's output at each step, as the entire source sentence is available. But, **padding masking** is applied to ignore the padded tokens in the encoder's output, ensuring that attention is only paid to valid (non-padded) tokens.\n",
    "\n",
    "However, **causal masking is used in the decoder's self-attention** when generating the French sentence. This ensures that each position can only attend to previous (or current) French words, preventing the model from \"seeing the future\" during generation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "68db3fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Attention Mask:\n",
      "tensor([[ True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False]])\n",
      "French Attention Mask:\n",
      "tensor([[ True,  True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True, False],\n",
      "        [ True,  True, False, False, False, False, False]])\n",
      "Final Output: torch.Size([3, 7, 18])\n"
     ]
    }
   ],
   "source": [
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, attn_p=0.0, proj_p=0.0, bias=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        if self.embed_dim % self.num_heads != 0:\n",
    "            raise ValueError(f\"Embedding dimension {self.embed_dim} must be divisible by number of heads {self.num_heads}\")\n",
    "        self.head_dim = self.embed_dim // self.num_heads\n",
    "\n",
    "        self.query = nn.Linear(self.embed_dim, self.embed_dim, bias = bias)\n",
    "        self.key = nn.Linear(self.embed_dim, self.embed_dim, bias = bias)\n",
    "        self.value = nn.Linear(self.embed_dim, self.embed_dim, bias = bias)\n",
    "        self.attn_drop = nn.Dropout(attn_p)\n",
    "\n",
    "        self.proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.proj_drop = nn.Dropout(proj_p)\n",
    "\n",
    "\n",
    "    def forward(self, src, tgt, attention_mask=None):\n",
    "\n",
    "        batch_size, src_seq_len, embed_dim = src.shape\n",
    "        _, tgt_seq_len, _ = tgt.shape\n",
    "        if embed_dim != self.embed_dim:\n",
    "            raise ValueError(f\"Input embedding dimension {embed_dim} does not match model's expected dimension {self.embed_dim}\")\n",
    "        \n",
    "        q = self.query(tgt).reshape(batch_size, tgt_seq_len, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        k = self.key(src).reshape(batch_size, src_seq_len, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        v = self.value(src).reshape(batch_size, src_seq_len, self.num_heads, self.head_dim).transpose(1,2)\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        \n",
    "        ### NOTE: \n",
    "        ### attn.shape - (Batch x num_heads x french_seq_len x english_seq_len)\n",
    "        ### mask.shape - (Batch x english_seq_len)\n",
    "        ### Need to expand mask (Batch x english_seq_len) -> (Batch x 1 x 1 x english_seq_len) -> (Batch x 1 x french_seq_len x english_seq_len)\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = attention_mask.unsqueeze(1).unsqueeze(1).repeat(1, 1, tgt_seq_len, 1)\n",
    "            attn = attn.masked_fill(~attention_mask, float(\"-inf\"))  # Apply the attention mask\n",
    "\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        # print(\"Attention Matrix after softmax:\\n\", attn)\n",
    "        attn = self.attn_drop(attn)\n",
    "        out = attn @ v\n",
    "\n",
    "        out = out.transpose(1, 2).reshape(batch_size, tgt_seq_len, self.embed_dim)\n",
    "        out = self.proj(out)\n",
    "        out = self.proj_drop(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "### We will now have sequences of different lengths, identify the number of tokens in each sequence ###\n",
    "english_seq_lens = [3,5,4]\n",
    "french_seq_lens = [7,6,2]\n",
    "\n",
    "embed_dim = 18\n",
    "num_heads = 3\n",
    "a = CrossAttention(embed_dim, num_heads)\n",
    "\n",
    "### Create random tensor in the shape (Batch x Seq Len x Embed Dim) for French and English ###\n",
    "### This will be a tensor upto the max(seq_lens) ###\n",
    "rand_english = torch.randn(len(english_seq_lens),max(english_seq_lens),embed_dim)\n",
    "rand_french = torch.randn(len(french_seq_lens),max(french_seq_lens),embed_dim)\n",
    "\n",
    "\n",
    "### Create Attention Mask from the seq_lens (shortest sequences padded to the longest ###\n",
    "english_masks = torch.nn.utils.rnn.pad_sequence([torch.ones(l) for l in english_seq_lens], batch_first=True, padding_value=0).bool()\n",
    "french_masks = torch.nn.utils.rnn.pad_sequence([torch.ones(l) for l in french_seq_lens], batch_first=True, padding_value=0).bool()\n",
    "\n",
    "print(\"English Attention Mask:\")\n",
    "print(english_masks)\n",
    "print(\"French Attention Mask:\")\n",
    "print(french_masks)\n",
    "\n",
    "### Pass through MHA ###\n",
    "output = a(src=rand_english, tgt=rand_french, attention_mask=english_masks)\n",
    "print(\"Final Output:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2317b34e",
   "metadata": {},
   "source": [
    "# Flash Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b50e78e",
   "metadata": {},
   "source": [
    "[Flash Attention](https://github.com/Dao-AILab/flash-attention) is a highly efficient implementation of the attention mechanism that fuses matrix multiplication, scaling, and softmax into a single CUDA kernel. This reduces memory overhead and speeds up computation, especially for long sequences.\n",
    "\n",
    "PyTorch provides access to Flash Attention via [`torch.nn.functional.scaled_dot_product_attention`](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html). This function expects:\n",
    "\n",
    "- **Queries**: `(B x H x L x E)`\n",
    "- **Keys**: `(B x H x S x E)`\n",
    "- **Values**: `(B x H x S x E)`\n",
    "- **attn_mask**: `(B x 1 x L x S)`, where `False` indicates positions to mask.\n",
    "\n",
    "It also has an `is_causal` flag to automatically apply causal masking.\n",
    "\n",
    "Flash Attention supports both self-attention (`L = S`) and cross-attention (`L â‰  S`), when queries are different from keys/values as we saw in cross-attention. The only extra step on our end is to make sure our ```attn_mask``` is of shape (B x 1 x L x S), which means we have to do the extra repeat, as it wont automatically broadcast along the $L$ dimension if we left it as (B x 1 x 1 x S)\n",
    "\n",
    "By merging operations and minimizing memory transfers, Flash Attention achieves significant speedups over standard attention implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5e2f17c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, embed_dim=768, num_heads=12, attn_drop=0.0, proj_drop=0.0, bias=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attn_drop = attn_drop\n",
    "        self.proj_drop = proj_drop\n",
    "\n",
    "        assert self.embed_dim % self.num_heads == 0, f\"Embedding dimension {self.embed_dim} must be divisible by number of heads {self.num_heads}\"\n",
    "        self.head_dim = self.embed_dim // self.num_heads\n",
    "\n",
    "        self.q_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=bias)\n",
    "        self.k_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=bias)\n",
    "        self.v_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=bias)\n",
    "        self.out_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=bias)\n",
    "\n",
    "\n",
    "    def forward(self, \n",
    "                src,\n",
    "                tgt=None,\n",
    "                attention_mask=None,\n",
    "                causal=False):\n",
    "        \n",
    "        batch, src_len, embed_dim = src.shape\n",
    "        \n",
    "        ### Self Attention ###\n",
    "        if tgt is None:\n",
    "            q = self.q_proj(src).reshape(batch, src_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "            k = self.k_proj(src).reshape(batch, src_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "            v = self.v_proj(src).reshape(batch, src_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "            if attention_mask is not None:\n",
    "                attention_mask = attention_mask.bool()\n",
    "                attention_mask = attention_mask.unsqueeze(1).unsqueeze(1).repeat(1, 1, src_len, 1)\n",
    "            \n",
    "        ### Cross Attention ###\n",
    "        else:\n",
    "            tgt_len = tgt.shape[1]\n",
    "            q = self.q_proj(tgt).reshape(batch, tgt_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "            k = self.k_proj(src).reshape(batch, src_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "            v = self.v_proj(src).reshape(batch, src_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "            if attention_mask is not None:\n",
    "                attention_mask = attention_mask.bool()\n",
    "                attention_mask = attention_mask.unsqueeze(1).unsqueeze(1).repeat(1, 1, tgt_len, 1)\n",
    "\n",
    "            causal = False # Causal masking is not used in cross attention\n",
    "\n",
    "\n",
    "        ##################################################################\n",
    "        attention_out = F.scaled_dot_product_attention(\n",
    "            q, k, v, \n",
    "            attn_mask=attention_mask,\n",
    "            dropout_p=self.attn_drop if self.train else 0.0,\n",
    "            is_causal=causal\n",
    "        )\n",
    "        ##################################################################\n",
    "\n",
    "\n",
    "        attention_out = attention_out.transpose(1, 2).flatten(2)  # (batch, num_heads, seq_len, head_dim) -> (batch, seq_len, num_heads * head_dim)\n",
    "        attention_out = self.out_proj(attention_out)\n",
    "        attention_out = F.dropout(attention_out, p=self.proj_drop, training=self.training)\n",
    "\n",
    "        return attention_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee86b1a",
   "metadata": {},
   "source": [
    "Testing Flash Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "80548505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING SELF-ATTENTION!!!\n",
      "-------------------------\n",
      "Attention Mask:\n",
      "tensor([[ True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False]])\n",
      "Final Output: torch.Size([3, 5, 9]) \n",
      "\n",
      "TESTING CROSS-ATTENTION!!!\n",
      "-------------------------\n",
      "English Attention Mask:\n",
      "tensor([[ True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False]])\n",
      "French Attention Mask:\n",
      "tensor([[ True,  True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True, False],\n",
      "        [ True,  True, False, False, False, False, False]])\n",
      "Final Output: torch.Size([3, 7, 9])\n"
     ]
    }
   ],
   "source": [
    "### Test Out Self-Attention!! ###\n",
    "print(\"TESTING SELF-ATTENTION!!!\")\n",
    "print(\"-------------------------\")\n",
    "seq_lens = [3,5,4]\n",
    "embed_dim = 9\n",
    "num_heads = 3\n",
    "a = Attention(embed_dim, num_heads)\n",
    "\n",
    "### Create a random tensor in the shape (Batch x Seq Len x Embed Dim) ###\n",
    "### This will be a tensor upto the max(seq_lens) ###\n",
    "rand = torch.randn(len(seq_lens),max(seq_lens),embed_dim)\n",
    "\n",
    "### Create Attention Mask from the seq_lens (shortest sequences padded to the longest ###\n",
    "masks = torch.nn.utils.rnn.pad_sequence([torch.ones(l) for l in seq_lens], batch_first=True, padding_value=0).bool()\n",
    "print(\"Attention Mask:\")\n",
    "print(masks)\n",
    "\n",
    "### Pass through MHA ###\n",
    "output = a(rand, attention_mask=masks, causal=True)\n",
    "print(\"Final Output:\", output.shape, \"\\n\")\n",
    "\n",
    "\n",
    "print(\"TESTING CROSS-ATTENTION!!!\")\n",
    "print(\"-------------------------\")\n",
    "### Test out Cross Attention \n",
    "### We will now have sequences of different lengths, identify the number of tokens in each sequence ###\n",
    "english_seq_lens = [3,5,4]\n",
    "french_seq_lens = [7,6,2]\n",
    "\n",
    "embed_dim = 9\n",
    "num_heads = 3\n",
    "a = Attention(embed_dim, num_heads)\n",
    "\n",
    "### Create random tensor in the shape (Batch x Seq Len x Embed Dim) for French and English ###\n",
    "### This will be a tensor upto the max(seq_lens) ###\n",
    "rand_english = torch.randn(len(english_seq_lens),max(english_seq_lens),embed_dim)\n",
    "rand_french = torch.randn(len(french_seq_lens),max(french_seq_lens),embed_dim)\n",
    "\n",
    "\n",
    "### Create Attention Mask from the seq_lens (shortest sequences padded to the longest ###\n",
    "english_masks = torch.nn.utils.rnn.pad_sequence([torch.ones(l) for l in english_seq_lens], batch_first=True, padding_value=0).bool()\n",
    "french_masks = torch.nn.utils.rnn.pad_sequence([torch.ones(l) for l in french_seq_lens], batch_first=True, padding_value=0).bool()\n",
    "\n",
    "print(\"English Attention Mask:\")\n",
    "print(english_masks)\n",
    "print(\"French Attention Mask:\")\n",
    "print(french_masks)\n",
    "\n",
    "### Pass through MHA ###\n",
    "output = a(src=rand_english, tgt=rand_french, attention_mask=english_masks)\n",
    "print(\"Final Output:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73114cd3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shardul",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
