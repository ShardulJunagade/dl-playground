{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words: A Step-by-Step Implementation\n",
    "This notebook demonstrates text preprocessing techniques including tokenization, stemming, lemmatization, and feature extraction using Bag of Words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I love sci-fi and am willing to put up with a lot. Sci-fi movies/TV are usually underfunded, under-appreciated and misunderstood. I tried to like this, I really did, but it is to good TV sci-fi as Babylon 5 is to Star Trek (the original). Silly prosthetics, cheap cardboard sets, stilted dialogues, CG that doesn't match the background, and painfully one-dimensional characters cannot be overcome with a 'sci-fi' setting. (I'm sure there are those of you out there who think Babylon 5 is good sci-fi TV. It's not. It's clichéd and uninspiring.)\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "review_text = (\n",
    "    \"I love sci-fi and am willing to put up with a lot. Sci-fi movies/TV are usually underfunded, \"\n",
    "    \"under-appreciated and misunderstood. I tried to like this, I really did, but it is to good TV sci-fi as Babylon 5 is to Star Trek \"\n",
    "    \"(the original). Silly prosthetics, cheap cardboard sets, stilted dialogues, CG that doesn't match the background, and painfully \"\n",
    "    \"one-dimensional characters cannot be overcome with a 'sci-fi' setting. (I'm sure there are those of you out there who think Babylon 5 is good sci-fi TV. \"\n",
    "    \"It's not. It's clichéd and uninspiring.)\"\n",
    ")\n",
    "display(review_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sentence Tokenization\n",
    "Tokenize the review into sentences using both NLTK and regex for comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences (NLTK):\n",
      " ['I love sci-fi and am willing to put up with a lot.', 'Sci-fi movies/TV are usually underfunded, under-appreciated and misunderstood.', 'I tried to like this, I really did, but it is to good TV sci-fi as Babylon 5 is to Star Trek (the original).', \"Silly prosthetics, cheap cardboard sets, stilted dialogues, CG that doesn't match the background, and painfully one-dimensional characters cannot be overcome with a 'sci-fi' setting.\", \"(I'm sure there are those of you out there who think Babylon 5 is good sci-fi TV.\", \"It's not.\", \"It's clichéd and uninspiring.)\"]\n"
     ]
    }
   ],
   "source": [
    "sentences_nltk = nltk.sent_tokenize(review_text)\n",
    "print(\"Sentences (NLTK):\\n\", sentences_nltk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences (Regex):\n",
      " ['I love sci-fi and am willing to put up with a lot.', 'Sci-fi movies/TV are usually underfunded, under-appreciated and misunderstood.', 'I tried to like this, I really did, but it is to good TV sci-fi as Babylon 5 is to Star Trek (the original).', \"Silly prosthetics, cheap cardboard sets, stilted dialogues, CG that doesn't match the background, and painfully one-dimensional characters cannot be overcome with a 'sci-fi' setting.\", \"(I'm sure there are those of you out there who think Babylon 5 is good sci-fi TV.\", \"It's not.\", \"It's clichéd and uninspiring.)\"]\n"
     ]
    }
   ],
   "source": [
    "sentences_regex = re.split(r\"(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s\", review_text)\n",
    "print(\"Sentences (Regex):\\n\", sentences_regex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Word Tokenization\n",
    "Tokenize the sentences into words using NLTK and custom regex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words (NLTK):\n",
      " ['I', 'love', 'sci-fi', 'and', 'am', 'willing', 'to', 'put', 'up', 'with', 'a', 'lot', '.', 'Sci-fi', 'movies/TV', 'are', 'usually', 'underfunded', ',', 'under-appreciated', 'and', 'misunderstood', '.', 'I', 'tried', 'to', 'like', 'this', ',', 'I', 'really', 'did', ',', 'but', 'it', 'is', 'to', 'good', 'TV', 'sci-fi', 'as', 'Babylon', '5', 'is', 'to', 'Star', 'Trek', '(', 'the', 'original', ')', '.', 'Silly', 'prosthetics', ',', 'cheap', 'cardboard', 'sets', ',', 'stilted', 'dialogues', ',', 'CG', 'that', 'does', \"n't\", 'match', 'the', 'background', ',', 'and', 'painfully', 'one-dimensional', 'characters', 'can', 'not', 'be', 'overcome', 'with', 'a', \"'sci-fi\", \"'\", 'setting', '.', '(', 'I', \"'m\", 'sure', 'there', 'are', 'those', 'of', 'you', 'out', 'there', 'who', 'think', 'Babylon', '5', 'is', 'good', 'sci-fi', 'TV', '.', 'It', \"'s\", 'not', '.', 'It', \"'s\", 'clichéd', 'and', 'uninspiring', '.', ')']\n"
     ]
    }
   ],
   "source": [
    "words_nltk = nltk.word_tokenize(review_text)\n",
    "print(\"Words (NLTK):\\n\", words_nltk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom regex tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words (Regex):\n",
      "['I', 'love', 'sci-fi', 'and', 'am', 'willing', 'to', 'put', 'up', 'with', 'a', 'lot']\n",
      "['Sci-fi', 'movies', 'TV', 'are', 'usually', 'underfunded', 'under-appreciated', 'and', 'misunderstood']\n",
      "['I', 'tried', 'to', 'like', 'this', 'I', 'really', 'did', 'but', 'it', 'is', 'to', 'good', 'TV', 'sci-fi', 'as', 'Babylon', '5', 'is', 'to', 'Star', 'Trek', 'the', 'original']\n",
      "['Silly', 'prosthetics', 'cheap', 'cardboard', 'sets', 'stilted', 'dialogues', 'CG', 'that', \"doesn't\", 'match', 'the', 'background', 'and', 'painfully', 'one-dimensional', 'characters', 'cannot', 'be', 'overcome', 'with', 'a', \"'sci-fi'\", 'setting']\n",
      "[\"I'm\", 'sure', 'there', 'are', 'those', 'of', 'you', 'out', 'there', 'who', 'think', 'Babylon', '5', 'is', 'good', 'sci-fi', 'TV']\n"
     ]
    }
   ],
   "source": [
    "def tokenise(sentence):\n",
    "    return re.findall(r\"[A-Z]{2,}(?![a-z])|[A-Z][a-z]+(?=[A-Z])|[\\'\\w\\-]+\", sentence)\n",
    "\n",
    "word_tokens_regex = [tokenise(sent) for sent in sentences_regex[:5]]\n",
    "print(\"Words (Regex):\")\n",
    "print(*word_tokens_regex, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Stemming\n",
    "Use the Porter Stemmer to reduce words to their root forms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed Sentences:\n",
      " ['love sci-fi will put lot .', 'sci-fi movies/tv usual underfund , under-appreci misunderstood .', 'tri like , realli , good tv sci-fi babylon 5 star trek ( origin ) .', \"silli prosthet , cheap cardboard set , stilt dialogu , cg n't match background , pain one-dimension charact overcom 'sci-fi ' set .\", \"( 'm sure think babylon 5 good sci-fi tv .\", \"'s .\", \"'s clichéd uninspir . )\"]\n"
     ]
    }
   ],
   "source": [
    "ps = PorterStemmer()\n",
    "\n",
    "stemmed_sentences = []\n",
    "for sentence in sentences_nltk:\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    stemmed = [ps.stem(word) for word in words if word.lower() not in stopwords.words(\"english\")]\n",
    "    stemmed_sentences.append(\" \".join(stemmed))\n",
    "\n",
    "print(\"Stemmed Sentences:\\n\", stemmed_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Lemmatization\n",
    "Use WordNet Lemmatizer to obtain base forms of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Sentences:\n",
      " ['love sci-fi willing put lot .', 'Sci-fi movies/TV usually underfunded , under-appreciated misunderstood .', 'tried like , really , good TV sci-fi Babylon 5 Star Trek ( original ) .', \"Silly prosthetics , cheap cardboard set , stilted dialogue , CG n't match background , painfully one-dimensional character overcome 'sci-fi ' setting .\", \"( 'm sure think Babylon 5 good sci-fi TV .\", \"'s .\", \"'s clichéd uninspiring . )\"]\n"
     ]
    }
   ],
   "source": [
    "wordnet = WordNetLemmatizer()\n",
    "\n",
    "lemmatized_sentences = []\n",
    "for sentence in sentences_nltk:\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    lemmatized = [wordnet.lemmatize(word) for word in words if word.lower() not in stopwords.words(\"english\")]\n",
    "    lemmatized_sentences.append(\" \".join(lemmatized))\n",
    "\n",
    "print(\"Lemmatized Sentences:\\n\", lemmatized_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Bag of Words Representation\n",
    "Create a Bag of Words representation for the processed sentences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love sci fi willing put lot', 'sci fi movie tv usually underfunded appreciated misunderstood', 'tried like really good tv sci fi babylon star trek original', 'silly prosthetics cheap cardboard set stilted dialogue cg match background painfully one dimensional character cannot overcome sci fi setting', 'sure think babylon good sci fi tv']\n"
     ]
    }
   ],
   "source": [
    "wordnet = WordNetLemmatizer()\n",
    "corpus = []\n",
    "\n",
    "for sentence in sentences_nltk:\n",
    "\treview = re.sub('[^a-zA-Z]', ' ', sentence)\t\t\t\t# substitute all non-alphabets with space\n",
    "\treview = review.lower()\n",
    "\twords = review.split()\n",
    "\tprocessed = [wordnet.lemmatize(word) for word in review if word not in set(stopwords.words('english'))]\n",
    "\tcorpus.append(' '.join(processed))\n",
    "\n",
    "print(corpus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Single Word (Unigram) Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram BOW:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>appreciated</th>\n",
       "      <th>babylon</th>\n",
       "      <th>background</th>\n",
       "      <th>cannot</th>\n",
       "      <th>cardboard</th>\n",
       "      <th>cg</th>\n",
       "      <th>character</th>\n",
       "      <th>cheap</th>\n",
       "      <th>dialogue</th>\n",
       "      <th>dimensional</th>\n",
       "      <th>...</th>\n",
       "      <th>star</th>\n",
       "      <th>stilted</th>\n",
       "      <th>sure</th>\n",
       "      <th>think</th>\n",
       "      <th>trek</th>\n",
       "      <th>tried</th>\n",
       "      <th>tv</th>\n",
       "      <th>underfunded</th>\n",
       "      <th>usually</th>\n",
       "      <th>willing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   appreciated  babylon  background  cannot  cardboard  cg  character  cheap  \\\n",
       "0            0        0           0       0          0   0          0      0   \n",
       "1            1        0           0       0          0   0          0      0   \n",
       "2            0        1           0       0          0   0          0      0   \n",
       "3            0        0           1       1          1   1          1      1   \n",
       "4            0        1           0       0          0   0          0      0   \n",
       "\n",
       "   dialogue  dimensional  ...  star  stilted  sure  think  trek  tried  tv  \\\n",
       "0         0            0  ...     0        0     0      0     0      0   0   \n",
       "1         0            0  ...     0        0     0      0     0      0   1   \n",
       "2         0            0  ...     1        0     0      0     1      1   1   \n",
       "3         1            1  ...     0        1     0      0     0      0   0   \n",
       "4         0            0  ...     0        0     1      1     0      0   1   \n",
       "\n",
       "   underfunded  usually  willing  \n",
       "0            0        0        1  \n",
       "1            1        1        0  \n",
       "2            0        0        0  \n",
       "3            0        0        0  \n",
       "4            0        0        0  \n",
       "\n",
       "[5 rows x 39 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unigram BOW\n",
    "cv = CountVectorizer(max_features=1500)\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "\n",
    "# Create DataFrame for visualization\n",
    "feature_names = cv.get_feature_names_out()\n",
    "df_unigram = pd.DataFrame(X, columns=feature_names)\n",
    "print(\"Unigram BOW:\")\n",
    "df_unigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Bigram Bag of Words\n",
    "As we move from unigram to bigram, the connection between mathematical similarity and intuitional similarity becomes tighter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram BOW:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>appreciated</th>\n",
       "      <th>appreciated misunderstood</th>\n",
       "      <th>babylon</th>\n",
       "      <th>babylon good</th>\n",
       "      <th>babylon star</th>\n",
       "      <th>background</th>\n",
       "      <th>background painfully</th>\n",
       "      <th>cannot</th>\n",
       "      <th>cannot overcome</th>\n",
       "      <th>cardboard</th>\n",
       "      <th>...</th>\n",
       "      <th>tried like</th>\n",
       "      <th>tv</th>\n",
       "      <th>tv sci</th>\n",
       "      <th>tv usually</th>\n",
       "      <th>underfunded</th>\n",
       "      <th>underfunded appreciated</th>\n",
       "      <th>usually</th>\n",
       "      <th>usually underfunded</th>\n",
       "      <th>willing</th>\n",
       "      <th>willing put</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   appreciated  appreciated misunderstood  babylon  babylon good  \\\n",
       "0            0                          0        0             0   \n",
       "1            1                          1        0             0   \n",
       "2            0                          0        1             0   \n",
       "3            0                          0        0             0   \n",
       "4            0                          0        1             1   \n",
       "\n",
       "   babylon star  background  background painfully  cannot  cannot overcome  \\\n",
       "0             0           0                     0       0                0   \n",
       "1             0           0                     0       0                0   \n",
       "2             1           0                     0       0                0   \n",
       "3             0           1                     1       1                1   \n",
       "4             0           0                     0       0                0   \n",
       "\n",
       "   cardboard  ...  tried like  tv  tv sci  tv usually  underfunded  \\\n",
       "0          0  ...           0   0       0           0            0   \n",
       "1          0  ...           0   1       0           1            1   \n",
       "2          0  ...           1   1       1           0            0   \n",
       "3          1  ...           0   0       0           0            0   \n",
       "4          0  ...           0   1       0           0            0   \n",
       "\n",
       "   underfunded appreciated  usually  usually underfunded  willing  willing put  \n",
       "0                        0        0                    0        1            1  \n",
       "1                        1        1                    1        0            0  \n",
       "2                        0        0                    0        0            0  \n",
       "3                        0        0                    0        0            0  \n",
       "4                        0        0                    0        0            0  \n",
       "\n",
       "[5 rows x 81 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bigram BOW\n",
    "cv_bigram = CountVectorizer(max_features=1500, ngram_range=(1, 2))\n",
    "X_bigram = cv_bigram.fit_transform(corpus).toarray()\n",
    "\n",
    "# Create DataFrame for visualization\n",
    "feature_names_bigram = cv_bigram.get_feature_names_out()\n",
    "df_bigram = pd.DataFrame(X_bigram, columns=feature_names_bigram)\n",
    "print(\"Bigram BOW:\")\n",
    "df_bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
